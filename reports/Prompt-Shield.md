# AI Harness Scorecard: Prompt-Shield

**Grade: F** (38.7/100) | No meaningful harness. AI output is essentially unaudited.

- **Repository**: `/tmp/Prompt-Shield`
- **Languages**: python
- **Assessed**: 2026-02-28 10:19 UTC
- **Checks**: 13/31 passed

## Summary

| Category | Weight | Score | Checks |
|----------|--------|-------|--------|
| Architectural Documentation | 20% | 25% [##--------] | 1/5 |
| Mechanical Constraints | 25% | 39% [####------] | 3/7 |
| Testing & Stability | 25% | 48% [#####-----] | 4/8 |
| Review & Drift Prevention | 15% | 60% [######----] | 4/6 |
| AI-Specific Safeguards | 15% | 20% [##--------] | 1/5 |

## Architectural Documentation (25%)

### [PASS] Architecture Documentation (5/5)

_matklad ARCHITECTURE.md guide_

**Evidence**: Found: docs/ARCHITECTURE.md

### [FAIL] Agent Instructions (0/5)

_OpenAI Harness Engineering (2026)_

**Evidence**: No AI agent instruction files found

**Remediation**: Create CLAUDE.md or AGENTS.md with project context, code style, and constraints so AI agents produce consistent output.

### [FAIL] Architecture Decision Records (0/3)

_DORA 2025 Report - AI-accessible documentation_

**Evidence**: No Architecture Decision Records found

**Remediation**: Create docs/adr/ directory with numbered markdown decision records. Use adr-tools or a simple template.

### [FAIL] Module Boundary Documentation (0/4)

_matklad ARCHITECTURE.md - constraints as absences_

**Evidence**: No module boundary constraints documented

**Remediation**: Document which modules must NOT depend on each other in ARCHITECTURE.md. Example: 'The fields crate never depends on any other workspace crate.'

### [FAIL] API Documentation (0/3)

_DORA 2025 - AI-accessible documentation_

**Evidence**: No API documentation generation or spec files found

**Remediation**: Add doc generation to CI (cargo doc, typedoc, sphinx) or maintain OpenAPI/Swagger specs.


## Mechanical Constraints (39%)

### [PASS] CI Pipeline (3/3)

_DORA 2025 Report_

**Evidence**: CI detected: github, github, github, github, github, github, github

### [FAIL] Linter Enforcement (0/4)

_OpenAI Harness Engineering - mechanical constraints_

**Evidence**: No linter found in CI

**Remediation**: Add a linter to CI that blocks merges on violations (e.g. cargo clippy -- -D warnings, eslint --max-warnings 0).

### [FAIL] Formatter Enforcement (0/3)

_OpenAI Harness Engineering - mechanical constraints_

**Evidence**: No formatter check found in CI

**Remediation**: Add a formatter check to CI (e.g. cargo fmt --all -- --check, prettier --check).

### [PASS] Type Safety (2/3)

_SlopCodeBench - preventing subtle type errors_

**Evidence**: Type checker configured but not confirmed in CI

**Remediation**: Add mypy/pyright to your CI pipeline.

### [PASS] Dependency Auditing (4/4)

_Blog: security infrastructure reliability_

**Evidence**: Blocking dependency audit in CI: dependabot

### [FAIL] Conventional Commits (0/2)

_DORA 2025 - working in small batches_

**Evidence**: No conventional commit enforcement found

**Remediation**: Add commitlint or equivalent to CI to enforce consistent commit message format.

### [FAIL] Unsafe Code Policy (0/3)

_Blog: 80% problem in AI-generated code_

**Evidence**: No explicit policy against unsafe code patterns

**Remediation**: Add unsafe_code = forbid (Rust), security linting (semgrep/bandit), or ESLint rules against dangerous patterns.


## Testing & Stability (48%)

### [PASS] Test Suite (3/3)

_Kent Beck - tests define what correct means_

**Evidence**: Tests present and executed in CI

### [PASS] Feature Matrix Testing (3/3)

_DORA 2025 - stability through comprehensive testing_

**Evidence**: Multiple test jobs in CI: test, docker, agent, agent, agent

### [PASS] Code Coverage (4/4)

_DORA 2025 - stability feedback loops_

**Evidence**: Coverage measurement in CI: coverage\.py|pytest-cov|--cov

### [FAIL] Mutation Testing (0/4)

_SlopCodeBench - code that 'appears correct but is unreliable'_

**Evidence**: No mutation testing found

**Remediation**: Add cargo-mutants (Rust), Stryker (JS/TS), mutmut (Python), or PIT (Java). Mutation testing catches tests that pass without verifying behavior.

### [FAIL] Property-Based Testing (0/3)

_Blog: catching edge cases in AI-generated code_

**Evidence**: No property-based testing found

**Remediation**: Add proptest (Rust), hypothesis (Python), or fast-check (JS/TS) for testing invariants with random structured inputs.

### [FAIL] Fuzz Testing (0/3)

_Blog: 80% problem - catching what AI misses_

**Evidence**: No fuzz testing found

**Remediation**: Add fuzz targets for parsing-heavy and input-handling code paths.

### [FAIL] Contract / Compatibility Tests (0/3)

_OpenAI Harness Engineering - mechanical constraints_

**Evidence**: No contract or compatibility tests found

**Remediation**: Add contract tests that verify external interface stability (golden fixtures, snapshot tests, wire-format checks).

### [PASS] Tests Block Merge (2/2)

_DORA 2025 - stability metrics_

**Evidence**: All test jobs are blocking: test, docker, agent


## Review & Drift Prevention (60%)

### [PASS] Code Review Required (2/4)

_OpenAI Harness Engineering - author/reviewer separation_

**Evidence**: Review-related configuration found in CI

**Remediation**: Enforce code review via branch protection rules (requires API access to verify).

### [PASS] Scheduled CI Jobs (3/3)

_OpenAI Harness Engineering - garbage collection agents_

**Evidence**: Scheduled CI pipeline found

### [PASS] Stale Documentation Detection (2/2)

_OpenAI Harness Engineering - quality drift_

**Evidence**: TODO/FIXME scanning found in CI

### [FAIL] PR/MR Template (0/2)

_DORA 2025 - working in small batches_

**Evidence**: No PR/MR template found

**Remediation**: Add .github/PULL_REQUEST_TEMPLATE.md or .gitlab/merge_request_templates/Default.md with sections for description, testing, and impact.

### [FAIL] Automated Code Review (0/2)

_OpenAI Harness Engineering - separate authoring and reviewing agents_

**Evidence**: No automated review tools found

**Remediation**: Configure CodeRabbit, SonarCloud, Dependabot/Renovate, or equivalent for automated review on every PR/MR.

### [PASS] Documentation Sync Check (2/2)

_OpenAI Harness Engineering - curated knowledge base_

**Evidence**: Doc sync check found in CI: doc.*sync


## AI-Specific Safeguards (20%)

### [FAIL] AI Usage Norms (0/4)

_DORA 2025 - clear organizational stance on AI use_

**Evidence**: No AI usage norms documented

**Remediation**: Document AI usage policies: review expectations for AI-generated code, when manual implementation is required, testing-before-implementation norms.

### [FAIL] Small Batch Enforcement (0/3)

_DORA 2025 - working in small batches_

**Evidence**: No small batch enforcement found

**Remediation**: Add PR size checks (Danger, pr-size-labeler) or document size guidelines in CONTRIBUTING.md. Large AI-generated PRs are harder to review.

### [PASS] Design-Before-Code Culture (3/3)

_Blog: cognitive offloading guardrails_

**Evidence**: Design/plan documents found: docs/DESIGN.md

### [FAIL] Error Handling Policy (0/3)

_Blog: AI agents deleting tests, using expect()_

**Evidence**: No error handling policy found

**Remediation**: Add clippy lints (unwrap_used, expect_used) for Rust, ESLint rules for JS/TS, or document error handling patterns in agent instructions.

### [FAIL] Security-Critical Path Marking (0/2)

_Blog: 80% problem in security infrastructure_

**Evidence**: No security-critical path marking found

**Remediation**: Add CODEOWNERS for sensitive directories, SECURITY.md for vuln reporting, or SAST scanning in CI.


## References

- Blog: 80% problem - catching what AI misses
- Blog: 80% problem in AI-generated code
- Blog: 80% problem in security infrastructure
- Blog: AI agents deleting tests, using expect()
- Blog: catching edge cases in AI-generated code
- Blog: cognitive offloading guardrails
- Blog: security infrastructure reliability
- DORA 2025 - AI-accessible documentation
- DORA 2025 - clear organizational stance on AI use
- DORA 2025 - stability feedback loops
- DORA 2025 - stability metrics
- DORA 2025 - stability through comprehensive testing
- DORA 2025 - working in small batches
- DORA 2025 Report
- DORA 2025 Report - AI-accessible documentation
- Kent Beck - tests define what correct means
- OpenAI Harness Engineering (2026)
- OpenAI Harness Engineering - author/reviewer separation
- OpenAI Harness Engineering - curated knowledge base
- OpenAI Harness Engineering - garbage collection agents
- OpenAI Harness Engineering - mechanical constraints
- OpenAI Harness Engineering - quality drift
- OpenAI Harness Engineering - separate authoring and reviewing agents
- SlopCodeBench - code that 'appears correct but is unreliable'
- SlopCodeBench - preventing subtle type errors
- matklad ARCHITECTURE.md - constraints as absences
- matklad ARCHITECTURE.md guide

---
*Generated by [ai-harness-scorecard](https://github.com/markmishaev/ai-harness-scorecard)*